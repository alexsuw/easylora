{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# easylora Quickstart\n",
    "\n",
    "This notebook demonstrates a minimal LoRA fine-tuning run using easylora.\n",
    "It uses a tiny model and a small synthetic dataset so it runs on CPU in under a minute.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexsuw/easylora/blob/main/notebooks/quickstart_colab.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install easylora (uncomment in Colab)\n",
    "# !pip install easylora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import easylora\n",
    "\n",
    "print(f\"easylora version: {easylora.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a tiny training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"tiny_train.jsonl\")\n",
    "examples = [\n",
    "    {\"text\": \"The quick brown fox jumps over the lazy dog.\"},\n",
    "    {\"text\": \"A simple test sentence for training.\"},\n",
    "    {\"text\": \"LoRA adapters are parameter efficient.\"},\n",
    "    {\"text\": \"Fine-tuning language models is fun.\"},\n",
    "]\n",
    "data_path.write_text(\"\\n\".join(json.dumps(ex) for ex in examples))\n",
    "print(f\"Created {data_path} with {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easylora import TrainConfig, train\n",
    "from easylora.config import (\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    OutputConfig,\n",
    "    TrainLoopConfig,\n",
    ")\n",
    "\n",
    "config = TrainConfig(\n",
    "    model=ModelConfig(\n",
    "        base_model=\"hf-internal-testing/tiny-random-LlamaForCausalLM\",\n",
    "        device_map=None,\n",
    "        torch_dtype=\"fp32\",\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset_path=str(data_path),\n",
    "        format=\"raw\",\n",
    "        max_seq_len=64,\n",
    "    ),\n",
    "    training=TrainLoopConfig(\n",
    "        epochs=1,\n",
    "        batch_size=2,\n",
    "        grad_accum=1,\n",
    "        max_steps=3,\n",
    "        logging_steps=1,\n",
    "        gradient_checkpointing=False,\n",
    "    ),\n",
    "    output=OutputConfig(\n",
    "        output_dir=\"./colab_output\",\n",
    "        allow_overwrite=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "artifacts = train(config)\n",
    "print(f\"\\nAdapter saved to: {artifacts.adapter_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = json.loads(Path(artifacts.summary_path).read_text())\n",
    "print(\"Training summary:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAdapter files:\")\n",
    "for f in sorted(Path(artifacts.adapter_dir).iterdir()):\n",
    "    size = f.stat().st_size\n",
    "    print(f\"  {f.name} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate text with the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easylora import load_adapter\n",
    "from easylora.config import ModelConfig\n",
    "from easylora.eval.generate import generate_samples\n",
    "from easylora.utils.hf import load_tokenizer\n",
    "\n",
    "model = load_adapter(\n",
    "    \"hf-internal-testing/tiny-random-LlamaForCausalLM\",\n",
    "    artifacts.adapter_dir,\n",
    "    device_map=None,\n",
    ")\n",
    "tokenizer = load_tokenizer(\n",
    "    ModelConfig(base_model=\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n",
    ")\n",
    "\n",
    "outputs = generate_samples(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [\"The quick brown\"],\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(f\"Generated: {outputs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"./colab_output\", ignore_errors=True)\n",
    "data_path.unlink(missing_ok=True)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
